{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical trick behing self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C= 2, 4, 3       # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Slicing Based on Dimensions\n",
    "For a tensor x of arbitrary shape (D1, D2, D3, ...):\n",
    "\n",
    "Select All Elements:<br/>\n",
    "x[:]: Selects the entire tensor. <br/>\n",
    "x[:, :, :]: Selects all elements across all dimensions. <br/>\n",
    "\n",
    "Single Dimension Slicing:<br/>\n",
    "x[start:stop]: Slices only along the first dimension.<br/>\n",
    "x[:, start:stop]: Slices along the second dimension.<br/>\n",
    "x[:, :, start:stop]: Slices along the third dimension.<br/>\n",
    "\n",
    "Specific Index Selection:<br/>\n",
    "x[index]: Selects a specific index along the first dimension, reducing dimensionality.<br/>\n",
    "x[:, index]: Selects a specific index along the second dimension.<br/>\n",
    "x[:, :, index]: Selects a specific index along the third dimension.<br/>\n",
    "\n",
    "Stepwise Slicing:<br/>\n",
    "x[::2]: Selects every second element along the first dimension.<br/>\n",
    "x[:, ::2]: Selects every second element along the second dimension.<br/>\n",
    "x[:, :, ::2]: Selects every second element along the third dimension.<br/>\n",
    "\n",
    "Reverse Slicing:<br/>\n",
    "x[::-1]: Reverses the first dimension.<br/>\n",
    "x[:, ::-1]: Reverses the second dimension.<br/>\n",
    "x[:, :, ::-1]: Reverses the third dimension.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want x[b, t] = mean_{i<=t} x[b, i]\n",
    "def batch_mean(B, T, C):\n",
    "    x = torch.randn(B, T, C)\n",
    "    xbatch_mean = torch.zeros((B, T, C))\n",
    "    for b in range(B):                          #this means for each batch\n",
    "        for t in range(T):                      #for every time frame or row\n",
    "            xprev= x[b, :t+1] #(t, C)           #slicing along time dimension, so the shape becomes of (t, C) as batch is constant due to for each batch\n",
    "            xbatch_mean[b, t] = torch.mean(xprev, axis= 0)   #mean along rows due to axis= 0\n",
    "    return x, xbatch_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, xbatch_mean = batch_mean(B=2, T=3, C=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1490,  0.1812, -0.0920,  1.5828],\n",
       "         [ 0.1526,  0.3843,  1.3091,  0.4645],\n",
       "         [-0.8345,  0.5978, -0.0514, -0.0646]],\n",
       "\n",
       "        [[-0.4970,  0.4658, -0.2573, -1.0673],\n",
       "         [ 2.0089, -0.5370,  0.2228,  0.6971],\n",
       "         [-1.4267,  0.9059,  0.1446,  0.2280]]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1490,  0.1812, -0.0920,  1.5828],\n",
       "         [ 0.6508,  0.2827,  0.6085,  1.0237],\n",
       "         [ 0.1557,  0.3878,  0.3886,  0.6609]],\n",
       "\n",
       "        [[-0.4970,  0.4658, -0.2573, -1.0673],\n",
       "         [ 0.7560, -0.0356, -0.0172, -0.1851],\n",
       "         [ 0.0284,  0.2782,  0.0367, -0.0474]]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbatch_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above we can see for every time step it is averaging its past time in channel dimension, in every batch \n",
    "like in batch 1 we can see first row remains same\n",
    "2 row becomes average of first and second row. and this row is the time.\n",
    "so on till last row\n",
    "\n",
    "and again same in batch 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "def MatMul(a, b):\n",
    "    c = a @ b \n",
    "    print(\"a=\")\n",
    "    print(a)\n",
    "    print(\"b=\")\n",
    "    print(b)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      "tensor([[6., 9.],\n",
      "        [9., 2.],\n",
      "        [8., 9.]])\n",
      "tensor([[23., 20.],\n",
      "        [23., 20.],\n",
      "        [23., 20.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "print(MatMul(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lower triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[1., 6., 1.],\n",
      "        [9., 0., 1.],\n",
      "        [9., 8., 4.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, dim= 1, keepdim= True)\n",
    "print(a)\n",
    "b = torch.randint(0,10, (3,3)).float()\n",
    "x_matmul_mean= MatMul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can conclude that the xbatch_mean and c are basically resulting similar, \n",
    "ie keeping the first time or row constant\n",
    "and from second row it is doing average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1490,  0.1812, -0.0920,  1.5828],\n",
       "         [ 0.1526,  0.3843,  1.3091,  0.4645],\n",
       "         [-0.8345,  0.5978, -0.0514, -0.0646]]),\n",
       " tensor([[ 1.1490,  0.1812, -0.0920,  1.5828],\n",
       "         [ 0.6508,  0.2827,  0.6085,  1.0237],\n",
       "         [ 0.1557,  0.3878,  0.3886,  0.6609]]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], xbatch_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 6., 1.],\n",
       "         [9., 0., 1.],\n",
       "         [9., 8., 4.]]),\n",
       " tensor([[1.0000, 6.0000, 1.0000],\n",
       "         [5.0000, 3.0000, 1.0000],\n",
       "         [6.3333, 4.6667, 2.0000]]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, x_matmul_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both xbatch_mean and x_matmul_mean is giving result with same logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C= 4, 8, 2       # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wei= torch.tril(torch.ones(T, T))\n",
    "wei= wei / wei.sum(dim= 1, keepdim= True)\n",
    "x_batch_mean= wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], x_batch_mean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Transformer researchpaper the attention is calculated using softmax, so lets try softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax\n",
    "tril= torch.tril(torch.ones(T, T))\n",
    "wei= torch.zeros((T, T))\n",
    "wei= wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei= F.softmax(wei, dim= -1)\n",
    "x_batch_mean_softmax = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], x_batch_mean_softmax[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also showing same result, as we wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#self attention\n",
    "\"\"\"\n",
    "the wei(weight) or tokens couldnot be uniform, above placed zeros.\n",
    "because different tokens finds different other tokens more or less interesting.\n",
    "and we want this to be data dependent, examples; vowels looks for the consonent in the past\n",
    "so now i want to gather the information from the past but i wanted to be in data dependent way.\n",
    "this is the problem that self attention solves.\n",
    "This is how self attention solves:\n",
    "for every single node or token at each position will two vector.\n",
    "query vector; what am i looking for\n",
    "key vector; what do i get\n",
    "so to get the affinities or similiraties between them, we dot product them.\n",
    "now that dot product between query and key gives the similarity between tokens.\n",
    "and here wei is that dot product.\n",
    "\"\"\"\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias= False)\n",
    "query = nn.Linear(C, head_size, bias= False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)      #(B, T, 16)\n",
    "q = query(x)    #(B, T, 16)\n",
    "wei = q @ k.transpose(-2,-1)  #(B,T,16) @ (B,16,T)--> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v= value(x)\n",
    "out = wei @ v\n",
    "\"\"\"\n",
    "we can think like x is like the private token. \n",
    "and value is if you find me relevant (based on the key), this is the information I’ll give you.\n",
    "\"\"\"\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we remove the masking of wei then it becomes the encoder, cause \n",
    "in decoder block there is autoregressive format where it predicts one token at a time based on previous context\n",
    "but in encoder we allow all the nodes or token to completely talk to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is self attention because key query and value are from same input x, cross attention means k and v from external source but query produced from input x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5572, 0.4428, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2462, 0.5681, 0.1857, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1756, 0.6245, 0.1141, 0.0858, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2107, 0.2477, 0.1696, 0.1438, 0.2283, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0845, 0.0158, 0.1586, 0.2427, 0.4899, 0.0084, 0.0000, 0.0000],\n",
       "        [0.1360, 0.1998, 0.1237, 0.1168, 0.0839, 0.1961, 0.1437, 0.0000],\n",
       "        [0.0401, 0.0075, 0.0564, 0.0686, 0.3650, 0.0103, 0.0306, 0.4215]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at tensor we can conclude, in one time frame the high value between tokens tells us that those token are of high affinity or finds each other more interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei= q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0632)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0564)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1176)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the reason of dividing it with sqrt of head_size is that, in weight initialization if the values are huge positive or huge negative then softmax vector will converge towards one hot vectors, which doesnot show data dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if the values are like this then look the softmax became too much peaky, for token 0.5\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we find token 0.3 and 0.5 are nearly same during softmax, so we want something like this \n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get the softmax values like this, we make the wei variance nearly equal to 1, by dividing it with sqrt of headsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead attention \n",
    "meaning multiple head of self attention running in parallel.\n",
    "run multiple head in parallel into a list and simply concatinate it over the channel dimension.\n",
    "\n",
    "since we have n_embed of size 32, if we have the the 4 channels then we have 8 dimensional self attention\n",
    "so from each communication channel we will have 8 dimensional vectors and those 4 channel concat to form the original n_embed size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward\n",
    "self attention is the communication, and once they gathered all the data they need to think on their data individually.\n",
    "and thats what feed forward does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block does communication followed by computation \n",
    "communication is done by multi-head attention\n",
    "computation is done by feed forward network on each token independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two optimization that help with the depth of this network\n",
    "Residual connections and layernorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual connection\n",
    "Residual connections in transformers ensure efficient gradient flow, prevent vanishing/exploding gradients, and allow the network to learn incrementally. This results in faster convergence, better optimization, and the ability to train deeper networks effectively. They help maintain stability and improve performance in the complex transformer architecture, especially as the model depth increases.\n",
    "\n",
    "In the self-attention mechanism, the process involves computing queries (Q), keys (K), and values (V), followed by an attention operation and a linear transformation. This results in a series of complex operations that can be difficult to optimize without residual connections.\n",
    "\n",
    "Skip Pathways for Attention Outputs: Each multi-head attention layer produces its own set of attention outputs, but instead of letting the output go through multiple transformations without control, a residual connection ensures that the input (the original query) is added back to the final output.\n",
    "\n",
    "The equation in the attention mechanism is typically:\n",
    "\n",
    "output=LayerNorm(QK^T +V)+input\n",
    "\n",
    "The output of the self-attention layer is added to the input to form the residual connection. This means that even if the attention mechanism fails to capture all useful information (e.g., poor attention weights), the original input \n",
    "x still contributes directly to the final output.\n",
    "\n",
    "This enables the model to retain useful information from previous layers and learn more efficiently because it has a stable reference to the input in every layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization:\n",
    "Batch normalization means across the batch dimension any individual neuron had unit Gaussian distribution. mean=0 and standard deviation=1\n",
    "and in layernorm normalizes the features, it normalizes row instead of column, \n",
    "\n",
    "Transformers typically use Layer Normalization after the attention and feed-forward layers. This helps improve optimization by stabilizing the training process. The residual connections, when combined with Layer Normalization, make sure the distribution of activations is stable and prevents the network from diverging.\n",
    "\n",
    "The equation for Layer Normalization with residual connections is:\n",
    "\n",
    "output=LayerNorm(input+F(input))\n",
    "\n",
    "Where \n",
    "F(input) represents the transformation (e.g., self-attention or feed-forward network).\n",
    "\n",
    "Improved Training Stability: The residual connection in each Transformer layer ensures that the model can learn incrementally rather than having to learn large transformations all at once. This incremental learning approach makes the optimization process more stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "Randomly preventing neighbout neurons to communicate by making it off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Connections in Deep Neural Networks\n",
    "\n",
    "When you have residual connections in a deep neural network, the layers are learning residuals, or differences between the input and output, rather than the entire transformation from input to output. This creates a shortcut path for the data to flow through the network.\n",
    "\n",
    "This shortcut is essentially a way for the network to learn identity mappings if necessary. Let me explain how that works:\n",
    "\n",
    "## 1. Deep Network Without Residuals (Problematic Case):\n",
    "In a very deep network without residual connections, the deeper layers need to learn the complete transformation \\( H(x) \\).\n",
    "As we add more layers, the learning process becomes harder, and the gradients may vanish or explode due to the increasing depth.\n",
    "This can cause the network to fail to learn effectively in deeper layers, and the performance could degrade as we add more layers. Essentially, the deep network might perform worse than a shallower network because each layer is having trouble learning useful features.\n",
    "\n",
    "## 2. Deep Network With Residuals (How it Helps):\n",
    "With residual connections, each layer doesn't need to learn the entire transformation from input to output. Instead, it only needs to learn the residual, i.e., the difference between the input and output:\n",
    "\n",
    "\\[\n",
    "y = F(x) + x\n",
    "\\]\n",
    "\n",
    "Where \\( F(x) \\) is the learned residual, and \\( x \\) is the input passed directly through.\n",
    "\n",
    "Now, if the network cannot learn anything useful in the deeper layers (e.g., if the gradient vanishes or the weights don’t adjust properly), the residual \\( F(x) \\) can just become zero:\n",
    "\n",
    "\\[\n",
    "F(x) = 0\n",
    "\\]\n",
    "\n",
    "Which leads to:\n",
    "\n",
    "\\[\n",
    "y = x\n",
    "\\]\n",
    "\n",
    "This means that the deeper layers would just output the input unchanged, essentially learning the identity function.\n",
    "\n",
    "## 3. What Does This Do for the Network's Performance?\n",
    "\n",
    "- **No Degradation**: When the deeper layers learn the identity function (i.e., \\( F(x) = 0 \\)), the output is the same as the input \\( y = x \\). This is equivalent to having a shallow network. In other words, the network would not perform worse than a shallow network.\n",
    "\n",
    "- **Avoiding Degradation**: The key point is that the residual network will not perform worse than its shallower counterpart. If the deeper layers fail to learn, they can simply pass the input through as it is, thus ensuring that the deeper network performs at least as well as the shallower one.\n",
    "\n",
    "- **Improvement in Training**: This flexibility allows the deeper network to continue training without the risk of degradation. The network can still learn useful features if the residual part \\( F(x) \\) is nonzero, but if not, it falls back to the identity function, which is still a valid solution. This helps avoid the network performance deteriorating as layers are added, which is the degradation problem.\n",
    "\n",
    "## Summary:\n",
    "- In a residual network, the deeper layers can always choose to learn the identity function (i.e., they can pass the input through unchanged).\n",
    "- This means that, even if the deeper layers cannot learn anything useful, the network will not perform worse than a shallow network.\n",
    "- The flexibility to learn the identity function prevents the network from suffering from performance degradation when more layers are added.\n",
    "- The network ensures at least the same performance as a shallow network, or even better if the deeper layers can learn useful transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Explained with Example\n",
    "\n",
    "### **1. Forward Pass**\n",
    "In the forward pass, we compute the output (`y_hat`) and the loss (`L`) based on the current weights and biases. The goal of this step is to obtain the predicted output so that we can calculate the error.\n",
    "\n",
    "- **What Happens:**\n",
    "  - Compute the linear combinations (`z[l] = W[l] * a[l-1] + b[l]`).\n",
    "  - Apply the activation function (`a[l] = f(z[l])`).\n",
    "  - Continue this process layer by layer until you reach the final output (`y_hat`).\n",
    "  - Calculate the loss (`L`) to measure the error between `y_hat` and the true label (`y`).\n",
    "\n",
    "- **Example Insight:**\n",
    "  If the network's prediction (`y_hat`) is far from the true value (`y`), the loss `L` will be large, indicating that significant updates to the weights are required.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Loss Gradient at the Output Layer**\n",
    "The first step in backpropagation is to calculate how the loss changes with respect to the output (`y_hat`).\n",
    "\n",
    "- **What Happens:**\n",
    "  - Compute the gradient of the loss with respect to `y_hat` (`∂L/∂y_hat`).\n",
    "  - This tells us how much the output is contributing to the error.\n",
    "\n",
    "- **Example Insight:**\n",
    "  If `L = 1/2 * (y_hat - y)^2`, then `∂L/∂y_hat = y_hat - y`.  \n",
    "  For `y_hat = 0.8` and `y = 1.0`, `∂L/∂y_hat = 0.8 - 1.0 = -0.2`.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Compute Error at the Output Layer**\n",
    "Next, the error at the output layer (`delta[L]`) is computed by incorporating the derivative of the activation function at the output.\n",
    "\n",
    "- **What Happens:**\n",
    "  - `delta[L] = ∂L/∂y_hat * f'(z[L])`, where `f'(z[L])` is the derivative of the activation function used in the output layer.\n",
    "  - This error term quantifies how much the output neuron needs to adjust its parameters to reduce the loss.\n",
    "\n",
    "- **Example Insight:**\n",
    "  For sigmoid activation:  \n",
    "  `f'(z) = f(z) * (1 - f(z))`.  \n",
    "  If `y_hat = 0.8`, then `f'(z[L]) = 0.8 * (1 - 0.8) = 0.16`.  \n",
    "  `delta[L] = -0.2 * 0.16 = -0.032`.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Compute Gradients for Weights and Biases at the Output Layer**\n",
    "Using the error `delta[L]`, compute how the weights and biases in the output layer affect the loss.\n",
    "\n",
    "- **What Happens:**\n",
    "  - Gradients for weights: `∂L/∂W[L] = delta[L] * a[L-1]`, where `a[L-1]` is the activation from the previous layer.\n",
    "  - Gradients for biases: `∂L/∂b[L] = delta[L]`.\n",
    "\n",
    "- **Example Insight:**\n",
    "  If `delta[L] = -0.032` and `a[L-1] = 0.75`, then:  \n",
    "  `∂L/∂W[L] = -0.032 * 0.75 = -0.024`,  \n",
    "  `∂L/∂b[L] = -0.032`.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Backpropagate Error to Hidden Layers**\n",
    "The error is propagated backward through the network. For hidden layers, we calculate the error using the chain rule.\n",
    "\n",
    "- **What Happens:**\n",
    "  - Error at layer `l`:  \n",
    "    `delta[l] = W[l+1]T * delta[l+1] * f'(z[l])`.\n",
    "  - This combines the contributions from the errors in the subsequent layer (`delta[l+1]`) and the derivative of the activation at the current layer.\n",
    "\n",
    "- **Example Insight:**\n",
    "  If `W[2] = 0.6`, `delta[2] = -0.032`, and `f'(z[1]) = 0.18`,  \n",
    "  `delta[1] = 0.6 * -0.032 * 0.18 = -0.003456`.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Compute Gradients for Weights and Biases in Hidden Layers**\n",
    "Using the errors in the hidden layers, compute the gradients for the weights and biases.\n",
    "\n",
    "- **What Happens:**\n",
    "  - Gradients for weights: `∂L/∂W[l] = delta[l] * a[l-1]`.\n",
    "  - Gradients for biases: `∂L/∂b[l] = delta[l]`.\n",
    "\n",
    "- **Example Insight:**\n",
    "  If `delta[1] = -0.003456` and `a[0] = x = 2.0`, then:  \n",
    "  `∂L/∂W[1] = -0.003456 * 2.0 = -0.006912`,  \n",
    "  `∂L/∂b[1] = -0.003456`.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Update Weights and Biases**\n",
    "Using the computed gradients, update the parameters using gradient descent.\n",
    "\n",
    "- **What Happens:**\n",
    "  - Update weights:  \n",
    "    `W[l] = W[l] - η * ∂L/∂W[l]`.\n",
    "  - Update biases:  \n",
    "    `b[l] = b[l] - η * ∂L/∂b[l]`.\n",
    "\n",
    "- **Example Insight:**\n",
    "  For `W[1] = 0.5`, `∂L/∂W[1] = -0.006912`, and `η = 0.1`:  \n",
    "  `W[1] = 0.5 - 0.1 * -0.006912 = 0.5006912`.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Repeat for Multiple Iterations**\n",
    "The process of forward pass, backward pass, and parameter updates is repeated for multiple epochs until the loss is minimized and the network predictions (`y_hat`) align closely with the true labels (`y`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Walkthrough (Simple Case)\n",
    "\n",
    "Consider the following:\n",
    "- Input: x = 2\n",
    "- Target output: y_true = 3\n",
    "- Initial weight: W = 0.5\n",
    "- Learning rate: eta = 0.1\n",
    "\n",
    "#### **Iteration 1:**\n",
    "\n",
    "1. **Forward Pass:**\n",
    "   - Predicted output: \n",
    "     y_pred = W * x = 0.5 * 2 = 1\n",
    "   - Loss:\n",
    "     Loss = 1/2 * (y_pred - y_true)^2 = 1/2 * (1 - 3)^2 = 2\n",
    "\n",
    "2. **Backward Pass (Gradient Computation):**\n",
    "   - Gradient of the loss with respect to W:\n",
    "     dL/dW = (y_pred - y_true) * 2 = (1 - 3) * 2 = -4\n",
    "\n",
    "3. **Weight Update:**\n",
    "   - Update weight W:\n",
    "     W = W - eta * dL/dW = 0.5 - 0.1 * (-4) = 0.5 + 0.4 = 0.9\n",
    "\n",
    "#### **Iteration 2:**\n",
    "\n",
    "1. **Forward Pass with Updated Weight:**\n",
    "   - Predicted output:\n",
    "     y_pred = W * x = 0.9 * 2 = 1.8\n",
    "   - Loss:\n",
    "     Loss = 1/2 * (y_pred - y_true)^2 = 1/2 * (1.8 - 3)^2 = 0.72\n",
    "\n",
    "2. **Backward Pass (Gradient Computation):**\n",
    "   - Gradient of the loss with respect to W:\n",
    "     dL/dW = (y_pred - y_true) * 2 = (1.8 - 3) * 2 = -2.4\n",
    "\n",
    "3. **Weight Update:**\n",
    "   - Update weight W:\n",
    "     W = 0.9 - 0.1 * (-2.4) = 0.9 + 0.24 = 1.14\n",
    "\n",
    "---\n",
    "\n",
    "This process repeats, with the weight getting closer to a value that minimizes the loss, ultimately making the model's predictions closer to the true labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
